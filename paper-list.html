<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Cong Fang"> 
<meta name="description" content="Cong Fang's home page">



<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Cong Fang</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
  
  </script>
</head>
<body >

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="680">

<p><font size=6><b>Cong Fang</b></font> </p>
				<p style="line-height:180%"><big><b>Assistant Professor</Large></big></b> </br>
				Peking University</br>
			
					Email: fangcong at pku dot edu dot cn 
				</p>
			</td>
			<td>
				<img src="fc.jpg" border="0" width="130">
			</td>
		<tr>
	</tbody>
</table>

<h2><A name="Full Paper List"><font color="black">Full Paper List</font></A> <small> <a href=topic.html>[By Topics</a>] [<a href="https://scholar.google.com/citations?user=N2M9RPoAAAAJ&hl=en">Google Scholar</a>][<a href=index.html>Back to Homepage</a>]</small></h2>
<ul>
	

	<li>
	 Scaling Law for Stochastic Gradient Descent in Quadratically Parameterized Linear Regression  [<a href="https://arxiv.org/abs/2502.09106">arXiv</a>]</br> 
	           Shihong Ding<sup>1</sup>, Haihan Zhang<sup>1</sup>, Hanzhen Zhao, <b>Cong Fang*</b>,</br>
                      
		   
	</li>



	<li>
	  Optimal Algorithms in Linear Regression under Covariate Shift: On the Importance of Precondition  [<a href="https://arxiv.org/abs/2502.09047">arXiv</a>]</br> 
	           Yuanshi Liu<sup>1</sup>, Haihan Zhang<sup>1</sup>, Qian Chen, <b>Cong Fang*</b>,</br>
                      
		   
	</li>


	<li>
	Fundamental Computational Limits in Pursuing Invariant Causal Prediction and Invariance-Guided Regularization  [<a href="https://arxiv.org/abs/2501.17354">arXiv</a>]</br> 
	              Yihong Gu, <b>Cong Fang</b>, Yang Xu, Zijian Guo, and Jianqing Fan,</br>
                      
	</li>




       <li>
	 Learning Curves of Stochastic Gradient Descent in Kernel Regression </br> 
	           Haihan Zhang<sup>1</sup>, Weicheng Lin<sup>1</sup>, Yuanshi Liu, <b>Cong Fang*</b>,</br>
                      
		   
	</li>



	<li>
	  Hessian-Aware Zeroth-Order Optimization [<a href="https://arxiv.org/abs/1812.11377">arXiv</a>]</br> 
	             Haishan Ye, Zhichao Huang, <b>Cong Fang</b>, Junchi Li, and Tong Zhang, </br>
                      
		    <em>IEEE Trans. on  Pattern Analysis and Machine Intelligence </em> (<b>TPAMI</b>), 2025.</br>  
	</li>



	<li>
	  SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process  [<a href="https://openreview.net/forum?id=8HuLgtjqOD">arXiv</a>]</br> 
	             Hanzhen Zhao, Xingyu Xie, <b>Cong Fang*</b>, and Zhouchen Lin, </br>
                      
		    <em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2025.</br>  
	</li>



	<li>
	   A Provable PArticle-based Primal-Dual ALgorithm for Mixed Nash Equilibrium  [<a href="https://arxiv.org/abs/2303.00970">arXiv</a>]</br> 
	             Shihong Ding<sup>1</sup>, Hanze Dong<sup>1</sup>, <b>Cong Fang*</b>, Zhouchen Lin, and Tong Zhang,</br>
                      
		    <em>Journal of Machine Learning Research </em> (<b>JMLR</b>), 2024.</br>  
	</li>



	<li>
			Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning  [<a href="https://arxiv.org/abs/2405.04715">arXiv</a>]</br> 
	              Yihong Gu, <b>Cong Fang</b>, Peter B&uumlhlmann, and Jianqing Fan,</br>
                      
	</li>


		<li> The Implicit Bias of Heterogeneity towards Invariance and Causality
			 [<a href="https://arxiv.org/abs/2403.01420">arXiv</a>]</br> 
			Yang Xu<sup>1</sup>, Yihong Gu<sup>1</sup>,  <b>Cong Fang*</b>,</br>
                         <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2024.</br>
		 
	        </li>

               	<li> Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition
			 [<a href="https://www.arxiv.org/abs/2407.14839">arXiv</a>]</br> 
			Shihong Ding, Long Yang, Luo Luo,  <b>Cong Fang*</b>,</br>
                         <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2024.</br>
		 
	        </li>



  	<li> Separation and Bias of Deep Equilibrium Models on Expressivity and Learning Dynamics
			</br> 
			Zhoutong Wu, Yimu Zhang,  <b>Cong Fang*</b>, Zhouchen Lin*</br>
                         <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2024.</br>
		 
	        </li>               




		<li>
			Accelerated Gradient Algorithms with Adaptive Subspace Search for Instance-Faster Optimization [<a href="https://arxiv.org/abs/2312.03218">arXiv</a>]</br> 
			Yuanshi Liu, Hanzhen Zhao,  Pengyun Yue,  <b>Cong Fang*</b>,</br>
		  
	</li>


	
			<li>
			Environment Invariant Linear Least Squares [<a href="https://arxiv.org/abs/2303.03092">arXiv</a>]</br> 
			Jianqing fan, <b>Cong Fang</b>, Yihong Gu, and Tong Zhang <small>(<b>&alpha;-&beta; order</b>)</small>,</br>
                        <em>Annals of Statistics </em> (<b>AoS</b>), 2024.</br>
		  
	</li>



        <li>
			Quantum Algorithms and Lower Bounds for Finite-Sum Optimization[<a href="https://arxiv.org/abs/2406.03006">arXiv</a>]</br> 
			Yexin Zhang, Chenyi Zhang,  <b>Cong Fang*</b>, Liwei Wang* and Tongyang Li*,</br>
                        <em>International Conference on Machine Learning </em> (<b>ICML</b>), 2024.</br>
		  
	</li>



	<li>
			INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations  [<a href="https://arxiv.org/abs/2403.12451">arXiv</a>]</br> 
			Luoli Rui, Guoxi Zhang,  Hongming Xu, Yaodong yang,  <b>Cong Fang*</b>, and Qing Li*,</br>
                        <em>International Conference on Machine Learning </em> (<b>ICML</b>), 2024.</br>
		  
	</li>


		<li>
			Relational Learning in Pre-Trained Models: A Theory from Hypergraph Recovery Perspective [<a href="https://arxiv.org/abs/2406.11249">arXiv</a>]</br> 
			Yang Chen, <b>Cong Fang*</b>, Zhouchen Lin*, and Bing Liu,</br>
                        <em>International Conference on Machine Learning </em> (<b>ICML</b>), 2024.</br>
		  
	</li>
             



	<li>

		
	Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee  </br> 
				Yuanshi Liu,  <b>Cong Fang*</b>, and  Tong Zhang, </br>
			  <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2023.</br>
	 
	</li>


	<li>

		
	Task-Robust Pre-Training for Worst-Case Downstream Adaptation   [<a href="https://arxiv.org/abs/2306.12070">arXiv</a>]</br> 
				Jianghui Wang<sup>1</sup>, Yang Chen<sup>1</sup>,  Xingyu Xie, <b>Cong Fang*</b>, and  Zhouchen Lin*, </br>
			  <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2023.</br>
	 
	</li>


	<li>

		
	Zeroth-order Optimization with Weak Dimension Dependency </br> 
				Pengyun Yue, Long Yang, <b>Cong Fang*</b>, and  Zhouchen Lin*, </br>
			  <em>Annual Conference on Learning Theory </em> (<b>COLT</b>), 2023.</br>
	 
	</li>

	<li>
		
	On the Lower Bound of Minimizing Polyak-≈Åojasiewicz Functions [<a href="https://arxiv.org/abs/2212.13551">arXiv</a>]</br> 
				Pengyun Yue, <b>Cong Fang*</b>, and Zhouchen Lin*, </br>
			  <em>Annual Conference on Learning Theory </em> (<b>COLT</b>), 2023.</br>
	 
	</li>


  	<li>
			Layer-Peeled Model: Toward Understanding Well-Trained Deep Neural Networks [<a href="https://arxiv.org/abs/2101.12699">arXiv</a>]</br> 
			<b>Cong Fang</b>, Hangfeng He, Qi Long, and Weijie Su <small>(<b>&alpha;-&beta; order</b>)</small>,</br>
	     	<em>Proceedings of the National Academy of Sciences </em> (<b>top journal: PNAS</b>), 2021, accepted. </br>
		
	</li>
<!-- <hr style="border:1px dashed #000; height:1px;margin-right:40pt;"></br> -->

	<li>

		
			Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Network [<a href="https://arxiv.org/abs/2007.01452">arXiv</a>]</br> 
					<b>Cong Fang</b>, Jason D. Lee, Pengkun Yang, and Tong Zhang <small>(<b>&alpha;-&beta; order</b>)</small>, </br>
			  <em>Annual Conference on Learning Theory </em> (<b>COLT</b>), 2021.</br>
	 
	</li>

	<li>
			Mathematical Models of Overparameterized Neural Networks
		[<a href="https://arxiv.org/abs/2012.13982">arXiv</a>]</br> 
			<b>Cong Fang</b>,  Hanze Dong, and Tong Zhang, </br>
		  <em>Proceedings of the IEEE </em> (<b>the flagship journal of IEEE: PIEEE</b>), 2021.
	</li>
	

	
	
	
			<li>
			How to Characterize the Landscape of Overparameterized Convolutional Neural Networks
			[<a href="https://papers.nips.cc/paper/2018/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html">paper</a>]</br> 
				Yihong Gu, Weizhong Zhang, <b>Cong Fang</b>, Jason D. Lee, and Tong Zhang,</br>
		  <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2020.</br>
	
	    </li>
	
	
		<li>
			Improved Analysis of Clipping Algorithms for Non-convex Optimization
			[<a href="https://proceedings.neurips.cc/paper/2020/hash/b282d1735283e8eea45bce393cefe265-Abstract.html">paper</a>][<a href="https://arxiv.org/abs/2010.02519">arXiv</a>]</br> 
			Bohang Zhang, Jikai Jin, <b>Cong Fang</b>, and  Liwei Wang,</br>
		  <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2020.</br>
	
	    </li>
	
     <li>
			Accelerated First-Order Optimization Algorithms for Machine Learning
		[<a href="https://ieeexplore.ieee.org/document/9146135">paper</a>]</br> 
			Huan  Li*, <b>Cong Fang*</b>,  and Zhouchen Lin  <small>(<b>*equal contribution</b>)</small>, </br>
		  <em>Proceedings of the IEEE </em> (<b>the flagship journal of IEEE: PIEEE</b>), 2020.
	</li>	
	
	
		<li>
			Decentralized Accelerated Gradient Methods With Increasing Penalty Parameters
			[<a href="https://ieeexplore.ieee.org/document/9173772">paper</a>][<a href="https://arxiv.org/abs/1810.01053">arXiv</a>]</br> 
			Huan Li, <b>Cong Fang</b>,   Zhouchen Lin, and Wotao Lin,</br>
		  <em>IEEE Trans. on Signal Processing </em> (<b>top signal processing journal: TSP</b>), 2020.</br>
		  
		  
		 		<li>
			Training Deep Neural Networks by Lifted Proximal Operator Machines
			[<a href="https://www.computer.org/csdl/journal/tp/5555/01/09311864/1q0ByMuDrkk">paper</a>]</br> 
			 Jia Li, Mingqing Xiao, <b>Cong Fang</b>,  Daiyue, Chao Xu, and Zhouchen Lin,</br>
		  <em>IEEE Trans. on  Pattern Analysis and Machine Intelligence </em> (<b>TPAMI</b>), 2020.</br> 
		  
	
	
		<li>
			Complexities in Projection-Free Stochastic Non-convex Minimization
			[<a href="http://proceedings.mlr.press/v89/shen19b.html">paper</a>]</br> 
			Zebang Shen, <b>Cong Fang</b>,  Peilin Zhao,  Junzhou Huang, and  Hui Qian,</br>
		  <em>The 22nd International Conference on Artificial Intelligence and Statistics </em> (<b>AISTATS</b>), 2019.</br>
	
	    </li>
	
	
	
		<li>
			Sharp Analysis for Nonconvex SGD Escaping from Saddle Points  [<a href="http://proceedings.mlr.press/v99/fang19a.html">paper</a>][<a href="https://arxiv.org/abs/1902.00247">arXiv</a>]</br> 
			<b>Cong Fang</b>, Zhouchen Lin, and Tong Zhang <small>(<b>&alpha;-&beta; order</b>)</small>,</br>
		  <em>Annual Conference on Learning Theory </em> (<b>COLT</b>), 2019.</br>
	
	</li>
	
		<li>
			Lifted Proximal Operator Machines
			[<a href="https://ojs.aaai.org//index.php/AAAI/article/view/4323">paper</a>][<a href="https://arxiv.org/abs/1811.01501">arXiv</a>]</br> 
			Jia Li, <b>Cong Fang</b>,  and Zhouchen Lin, </br>
		  <em>Thirty-Second AAAI Conference on Artificial Intelligence</em> (<b>AAAI</b>), 2018.</br>
	
	      </li>
	
	
		<li>
			SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator
			[<a href="https://papers.nips.cc/paper/2018/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html">paper</a>][<a href="https://arxiv.org/abs/1807.01695">arXiv</a>]</br> 
			<b>Cong Fang</b>,  Chris Junchi Li, Zhouchen Lin, and Tong Zhang <small>(<b>&alpha;-&beta; order</b>)</small>,</br>
		  <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2018.</br>
	
	    </li>
		
		
				<li>
			Dictionary learning with structured noise
			[<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231217313437">paper</a>]</br> 
			Pan Zhou, <b>Cong Fang</b>,  Zhouchen Lin, Chao Zhang, and  Edward Chang,</br>
		  <em>Neurocomputing</em>, 2018.</br>
	
	      </li>
	
			<li>
			Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers
			[<a href="https://proceedings.neurips.cc/paper/2017/hash/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Abstract.html">paper</a>]</br> 
			<b>Cong Fang</b>,  Feng Cheng, and  Zhouchen Lin,</br>
		 <em>Advances in Neural Information Processing Systems </em> (<b>NeurIPS</b>), 2017.</br>
	
	    </li>
		
					<li>
			Parallel Asynchronous Stochastic Variance Reduction for Nonconvex Optimization
			[<a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14433">paper</a>]</br> 
			<b>Cong Fang</b> and  Zhouchen Lin,</br>
		 <em>Thirty-First AAAI Conference on Artificial Intelligence </em> (<b>AAAI</b>), 2017.</br>
	
	    </li>
	
	    <li>
			Feature Learning via Partial Differential Equation with Applications to Face Recognition
			[<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231217313437">paper</a>]</br> 
			<b>Cong Fang</b>,  Zhenyu Zhao, Pan Zhou, and  Zhouchen Lin,</br>
		   <em>Pattern Recognition </em> (<b>PR</b>), 2017.</br>
	
	      </li>

	     <li>
			A Robust Hybrid Method for Text Detection in Natural Scenes by Learning-based Partial Differential Equations
			[<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231215008413">paper</a>]</br> 
			Zhenyu Zhao, <b>Cong Fang</b>, Zhouchen Lin, and Yi Wu,</br>
		   <em>Neurocomputing</em>, 2015.</br>
	
	      </li>

	

</ul>


<h2><A name="Books"><font color="black">Books</font></A></h2>
<ul>
	<li>
			Accelerated Optimization in Machine Learning: First-Order Algorithms [<a href="https://www.springer.com/gp/book/9789811529092">book</a>]</br> 
			Zhouchen Lin, Huan Li, and <b>Cong Fang</b>, Springer, 2020.</br>		 
		 <em>I am in charge of introducing stochastic and distributed algorithms (Chapters 5 and 6)<em>
	</li>
</ul>




<ul>
	<li>
			Alternating Direction Method of Multipliers for Machine Learning [<a href="https://link.springer.com/book/10.1007/978-981-16-9840-8">book</a>]</br> 
			Zhouchen Lin, Huan Li, and <b>Cong Fang</b>, Springer, 2020.</br>		 
		 <em>I am in charge of introducing stochastic  algorithm (Chapter 5)<em>
	</li>
</ul>






<div id="footer">
	<div id="footer-text"></div>
</div>
</div>
</body>
</html>
